{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "6f-XjrnFjW1e"
   },
   "source": [
    "# Image Classification with CIFAR-10 dataset\n",
    "\n",
    "Some of the code and description of this notebook is borrowed by [this repo](https://github.com/udacity/deep-learning/tree/master/image-classification)\n",
    "\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xEMtQvd9jW1j"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CIFAR-10 Dataset: 171MB [00:37, 4.60MB/s]                              \n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm \n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "class DownloadProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "\"\"\" \n",
    "    check if the data (zip) file is already downloaded\n",
    "    if not, download it from \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\" and save as cifar-10-python.tar.gz\n",
    "\"\"\"\n",
    "if not isfile('cifar-10-python.tar.gz'):\n",
    "    with DownloadProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            'cifar-10-python.tar.gz',\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open('cifar-10-python.tar.gz') as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ryp6RtKCjW1u"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WNlJHetrjW11"
   },
   "outputs": [],
   "source": [
    "def load_label_names():\n",
    "    return ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "re3pi_HNjW16"
   },
   "source": [
    "### How to reshape into a such form?\n",
    "\n",
    "The row vector (3072) has the exact same number of elements if you calculate 32\\*32\\*3==3072. In order to reshape the row vector, (3072)\n",
    "\n",
    "1. divide the row vector (3072) into 3 pieces. Each piece corresponds to the each channels.\n",
    "  - this results in (3 x 1024) dimension of tensor\n",
    "2. divide the resulting tensor from the previous step with 32. 32 here means width of an image.\n",
    "  - this results in (3 x 32 x 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DfO1A8lVjW17"
   },
   "outputs": [],
   "source": [
    "def load_cfar10_batch(cifar10_dataset_folder_path, batch_id):\n",
    "    with open(cifar10_dataset_folder_path + '/data_batch_' + str(batch_id), mode='rb') as file:\n",
    "        # note the encoding type is 'latin1'\n",
    "        batch = pickle.load(file, encoding='latin1')\n",
    "        \n",
    "    features = batch['data'].reshape((len(batch['data']), 3, 32, 32)).transpose(0, 2, 3, 1)\n",
    "    labels = batch['labels']\n",
    "        \n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PaMtopsQjW2D"
   },
   "source": [
    "## Explore the Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vZprpfQujW2G"
   },
   "outputs": [],
   "source": [
    "def display_stats(cifar10_dataset_folder_path, batch_id, sample_id):\n",
    "    features, labels = load_cfar10_batch(cifar10_dataset_folder_path, batch_id)\n",
    "    \n",
    "    if not (0 <= sample_id < len(features)):\n",
    "        print('{} samples in batch {}.  {} is out of range.'.format(len(features), batch_id, sample_id))\n",
    "        return None\n",
    "\n",
    "    print('\\nStats of batch #{}:'.format(batch_id))\n",
    "    print('# of Samples: {}\\n'.format(len(features)))\n",
    "    \n",
    "    label_names = load_label_names()\n",
    "    label_counts = dict(zip(*np.unique(labels, return_counts=True)))\n",
    "    for key, value in label_counts.items():\n",
    "        print('Label Counts of [{}]({}) : {}'.format(key, label_names[key].upper(), value))\n",
    "    \n",
    "    sample_image = features[sample_id]\n",
    "    sample_label = labels[sample_id]\n",
    "    \n",
    "    print('\\nExample of Image {}:'.format(sample_id))\n",
    "    print('Image - Min Value: {} Max Value: {}'.format(sample_image.min(), sample_image.max()))\n",
    "    print('Image - Shape: {}'.format(sample_image.shape))\n",
    "    print('Label - Label Id: {} Name: {}'.format(sample_label, label_names[sample_label]))\n",
    "    \n",
    "    plt.imshow(sample_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 588
    },
    "colab_type": "code",
    "id": "VhOqopY7jW2L",
    "outputId": "999d73df-c329-40da-cd95-71cf0a5f5cab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch #3:\n",
      "# of Samples: 10000\n",
      "\n",
      "Label Counts of [0](AIRPLANE) : 994\n",
      "Label Counts of [1](AUTOMOBILE) : 1042\n",
      "Label Counts of [2](BIRD) : 965\n",
      "Label Counts of [3](CAT) : 997\n",
      "Label Counts of [4](DEER) : 990\n",
      "Label Counts of [5](DOG) : 1029\n",
      "Label Counts of [6](FROG) : 978\n",
      "Label Counts of [7](HORSE) : 1015\n",
      "Label Counts of [8](SHIP) : 961\n",
      "Label Counts of [9](TRUCK) : 1029\n",
      "\n",
      "Example of Image 7000:\n",
      "Image - Min Value: 24 Max Value: 252\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 0 Name: airplane\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfMAAAHxCAYAAAB5x1VAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJUnK3P8HDVTlK+4umqsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff3b672f208>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 248,
       "width": 249
      },
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 3\n",
    "sample_id = 7000\n",
    "display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vocUN8mejW2U"
   },
   "source": [
    "## Implement Preprocess Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I27zvPRqjW2X"
   },
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "        argument\n",
    "            - x: input image data in numpy array [32, 32, 3]\n",
    "        return\n",
    "            - normalized x \n",
    "    \"\"\"\n",
    "    min_val = np.min(x)\n",
    "    max_val = np.max(x)\n",
    "    x = (x-min_val) / (max_val-min_val)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FLiyd4C_jW2d"
   },
   "source": [
    "### One-hot encode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NX4K9aKgjW2g"
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "        argument\n",
    "            - x: a list of labels\n",
    "        return\n",
    "            - one hot encoding matrix (number of labels, number of class)\n",
    "    \"\"\"\n",
    "    encoded = np.zeros((len(x), 10))\n",
    "    \n",
    "    for idx, val in enumerate(x):\n",
    "        encoded[idx][val] = 1\n",
    "    \n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m0Uqf_4DjW2k"
   },
   "source": [
    "## Preprocess all the data and save it\n",
    "\n",
    "The code cell below uses the previously implemented functions, normalize and one_hot_encode, to preprocess the given dataset.\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eX_fqG9CjW2l"
   },
   "outputs": [],
   "source": [
    "def _preprocess_and_save(normalize, one_hot_encode, features, labels, filename):\n",
    "    features = normalize(features)\n",
    "    labels = one_hot_encode(labels)\n",
    "\n",
    "    pickle.dump((features, labels), open(filename, 'wb'))\n",
    "\n",
    "\n",
    "def preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode):\n",
    "    n_batches = 5\n",
    "    valid_features = []\n",
    "    valid_labels = []\n",
    "\n",
    "    for batch_i in range(1, n_batches + 1):\n",
    "        features, labels = load_cfar10_batch(cifar10_dataset_folder_path, batch_i)\n",
    "        \n",
    "        # find index to be the point as validation data in the whole dataset of the batch (10%)\n",
    "        index_of_validation = int(len(features) * 0.1)\n",
    "\n",
    "        # preprocess the 90% of the whole dataset of the batch\n",
    "        # - normalize the features\n",
    "        # - one_hot_encode the lables\n",
    "        # - save in a new file named, \"preprocess_batch_\" + batch_number\n",
    "        # - each file for each batch\n",
    "        _preprocess_and_save(normalize, one_hot_encode,\n",
    "                             features[:-index_of_validation], labels[:-index_of_validation], \n",
    "                             'preprocess_batch_' + str(batch_i) + '.p')\n",
    "\n",
    "        # unlike the training dataset, validation dataset will be added through all batch dataset\n",
    "        # - take 10% of the whold dataset of the batch\n",
    "        # - add them into a list of\n",
    "        #   - valid_features\n",
    "        #   - valid_labels\n",
    "        valid_features.extend(features[-index_of_validation:])\n",
    "        valid_labels.extend(labels[-index_of_validation:])\n",
    "\n",
    "    # preprocess the all stacked validation dataset\n",
    "    _preprocess_and_save(normalize, one_hot_encode,\n",
    "                         np.array(valid_features), np.array(valid_labels),\n",
    "                         'preprocess_validation.p')\n",
    "\n",
    "    # load the test dataset\n",
    "    with open(cifar10_dataset_folder_path + '/test_batch', mode='rb') as file:\n",
    "        batch = pickle.load(file, encoding='latin1')\n",
    "\n",
    "    # preprocess the testing data\n",
    "    test_features = batch['data'].reshape((len(batch['data']), 3, 32, 32)).transpose(0, 2, 3, 1)\n",
    "    test_labels = batch['labels']\n",
    "\n",
    "    # Preprocess and Save all testing data\n",
    "    _preprocess_and_save(normalize, one_hot_encode,\n",
    "                         np.array(test_features), np.array(test_labels),\n",
    "                         'preprocess_training.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9xSIZ8VEjW2n"
   },
   "outputs": [],
   "source": [
    "preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NgEAErXIjW2q"
   },
   "source": [
    "## Checkpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uVT1R0jRjW2q"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5IQcjPSejW2u"
   },
   "source": [
    "### Prepare Input for the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VSI9GRmljW2v"
   },
   "outputs": [],
   "source": [
    "# Remove previous weights, bias, inputs, etc..\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = tf.placeholder(tf.float32, shape=(None, 32, 32, 3), name='input_x')\n",
    "y =  tf.placeholder(tf.float32, shape=(None, 10), name='output_y')\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WqwoYrh4jW21"
   },
   "source": [
    "### Create Convolutional Model\n",
    "\n",
    "The entire model consists of 9 layers in total. In addition to layers below lists what techniques are applied to build the model.\n",
    "\n",
    "1. Convolution with 64 different filters in size of (3x3)\n",
    "2. Max Pooling\n",
    "  - ReLU activation function \n",
    "  - Batch Normalization\n",
    "3. Convolution with 128 different filters in size of (3x3)\n",
    "  - Dropout \n",
    "4. Max Pooling\n",
    "  - ReLU activation function \n",
    "  - Batch Normalization\n",
    "5. Convolution with 256 different filters in size of (3x3)\n",
    "6. Max Pooling\n",
    "  - ReLU activation function \n",
    "  - Batch Normalization\n",
    "7. Convolution with 512 different filters in size of (3x3)\n",
    "  - Dropout \n",
    "8. Max Pooling\n",
    "  - ReLU activation function \n",
    "  - Batch Normalization \n",
    "9. Flattening the 3-D output of the last convolutional operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0nnsILOpjW23"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def conv_net(x, keep_prob):\n",
    "    conv1_filter = tf.Variable(tf.truncated_normal(shape=[3, 3, 3, 64], mean=0, stddev=0.08))\n",
    "    conv2_filter = tf.Variable(tf.truncated_normal(shape=[3, 3, 64, 128], mean=0, stddev=0.08))\n",
    "    conv3_filter = tf.Variable(tf.truncated_normal(shape=[5, 5, 128, 256], mean=0, stddev=0.08))\n",
    "    conv4_filter = tf.Variable(tf.truncated_normal(shape=[5, 5, 256, 512], mean=0, stddev=0.08))\n",
    "\n",
    "    # 1, 2\n",
    "    conv1 = tf.nn.conv2d(x, conv1_filter, strides=[1,1,1,1], padding='SAME')\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "    conv1_pool = tf.nn.max_pool(conv1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "    conv1_bn = tf.layers.batch_normalization(conv1_pool)\n",
    "    conv1_drop = tf.nn.dropout(conv1_bn, keep_prob)\n",
    "    \n",
    "    # 3, 4\n",
    "    conv2 = tf.nn.conv2d(conv1_drop, conv2_filter, strides=[1,1,1,1], padding='SAME')\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "    conv2_pool = tf.nn.max_pool(conv2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')    \n",
    "    conv2_bn = tf.layers.batch_normalization(conv2_pool)\n",
    "    conv2_drop = tf.nn.dropout(conv2_bn, keep_prob)\n",
    "  \n",
    "    # 5, 6\n",
    "    conv3 = tf.nn.conv2d(conv2_bn, conv3_filter, strides=[1,1,1,1], padding='SAME')\n",
    "    conv3 = tf.nn.relu(conv3)\n",
    "    conv3_pool = tf.nn.max_pool(conv3, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')  \n",
    "    conv3_bn = tf.layers.batch_normalization(conv3_pool)\n",
    "    conv3_drop = tf.nn.dropout(conv3_bn, keep_prob)\n",
    "    \n",
    "    # 7, 8\n",
    "    conv4 = tf.nn.conv2d(conv3_bn, conv4_filter, strides=[1,1,1,1], padding='SAME')\n",
    "    conv4 = tf.nn.relu(conv4)\n",
    "    conv4_pool = tf.nn.max_pool(conv4, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "    conv4_bn = tf.layers.batch_normalization(conv4_pool)\n",
    "    conv4_drop = tf.nn.dropout(conv4_bn, keep_prob)\n",
    "    \n",
    "    # 9\n",
    "    flat = tf.contrib.layers.flatten(conv4_drop)  \n",
    "\n",
    "    # 10\n",
    "    out = tf.contrib.layers.fully_connected(inputs=flat, num_outputs=10, activation_fn=None)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gIxvOLJPjW27"
   },
   "source": [
    "### Hyperparameters\n",
    "\n",
    "* `epochs`: number of iterations until the network stops learning or start overfitting\n",
    "* `batch_size`: highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    "* `keep_probability`: probability of keeping a node using dropout\n",
    "* `learning_rate`: number how fast the model learns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vk5nE8ypjW28"
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 128\n",
    "keep_probability = 0.7\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "colab_type": "code",
    "id": "b-1dWgABjW3B",
    "outputId": "2ea527de-b03e-4626-eebe-baf8e39297f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-35-bc29ffa8bb57>:5: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logits = conv_net(x, keep_prob)\n",
    "model = tf.identity(logits, name='logits') # Name logits Tensor, so that can be loaded from disk after training\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "74jBeS2jjW3E"
   },
   "source": [
    "## Train the Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "46LnkEkYjW3F"
   },
   "outputs": [],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    session.run(optimizer, \n",
    "                feed_dict={\n",
    "                    x: feature_batch,\n",
    "                    y: label_batch,\n",
    "                    keep_prob: keep_probability\n",
    "                })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-7rikop3jW3H"
   },
   "source": [
    "### Show Stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GJ7yqGw2jW3I"
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    loss = sess.run(cost, \n",
    "                    feed_dict={\n",
    "                        x: feature_batch,\n",
    "                        y: label_batch,\n",
    "                        keep_prob: 1.\n",
    "                    })\n",
    "    valid_acc = sess.run(accuracy, \n",
    "                         feed_dict={\n",
    "                             x: valid_features,\n",
    "                             y: valid_labels,\n",
    "                             keep_prob: 1.\n",
    "                         })\n",
    "    \n",
    "    print('Loss: {:>10.4f} Validation Accuracy: {:.6f}'.format(loss, valid_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zlO7cAa2jW3M"
   },
   "source": [
    "### Fully Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ambDMefOjW3M"
   },
   "outputs": [],
   "source": [
    "def batch_features_labels(features, labels, batch_size):\n",
    "    \"\"\"\n",
    "    Split features and labels into batches\n",
    "    \"\"\"\n",
    "    for start in range(0, len(features), batch_size):\n",
    "        end = min(start + batch_size, len(features))\n",
    "        yield features[start:end], labels[start:end]\n",
    "\n",
    "def load_preprocess_training_batch(batch_id, batch_size):\n",
    "    \"\"\"\n",
    "    Load the Preprocessed Training data and return them in batches of <batch_size> or less\n",
    "    \"\"\"\n",
    "    filename = 'preprocess_batch_' + str(batch_id) + '.p'\n",
    "    features, labels = pickle.load(open(filename, mode='rb'))\n",
    "\n",
    "    # Return the training data in batches of size <batch_size> or less\n",
    "    return batch_features_labels(features, labels, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 884
    },
    "colab_type": "code",
    "id": "m1xIf2wijW3P",
    "outputId": "0cd7c559-a6eb-4d20-dcd1-d78eaf04b8bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:     2.2495 Validation Accuracy: 0.256200\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss:     2.2704 Validation Accuracy: 0.182000\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss:     1.9188 Validation Accuracy: 0.321800\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss:     1.8917 Validation Accuracy: 0.382600\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss:     1.4161 Validation Accuracy: 0.500200\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:     1.4422 Validation Accuracy: 0.560000\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss:     1.4898 Validation Accuracy: 0.466400\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss:     1.2394 Validation Accuracy: 0.527000\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss:     1.1714 Validation Accuracy: 0.565000\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss:     1.0176 Validation Accuracy: 0.585200\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:     0.9847 Validation Accuracy: 0.677800\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss:     0.9526 Validation Accuracy: 0.594000\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss:     0.7789 Validation Accuracy: 0.588400\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss:     0.7376 Validation Accuracy: 0.627000\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss:     0.6382 Validation Accuracy: 0.663000\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:     0.5730 Validation Accuracy: 0.706600\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss:     0.6230 Validation Accuracy: 0.682600\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss:     0.5219 Validation Accuracy: 0.646800\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss:     0.3458 Validation Accuracy: 0.707600\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss:     0.3809 Validation Accuracy: 0.682000\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:     0.2401 Validation Accuracy: 0.735000\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss:     0.2884 Validation Accuracy: 0.744000\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss:     0.2694 Validation Accuracy: 0.674400\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss:     0.1538 Validation Accuracy: 0.728600\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss:     0.2628 Validation Accuracy: 0.695400\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:     0.1464 Validation Accuracy: 0.726800\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss:     0.1494 Validation Accuracy: 0.728400\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss:     0.1376 Validation Accuracy: 0.685800\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss:     0.0682 Validation Accuracy: 0.745600\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss:     0.2747 Validation Accuracy: 0.705000\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:     0.0956 Validation Accuracy: 0.732600\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss:     0.2054 Validation Accuracy: 0.718400\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss:     0.0969 Validation Accuracy: 0.710200\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss:     0.0837 Validation Accuracy: 0.721800\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss:     0.0483 Validation Accuracy: 0.748200\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:     0.0640 Validation Accuracy: 0.724000\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss:     0.1334 Validation Accuracy: 0.701600\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss:     0.0178 Validation Accuracy: 0.731600\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss:     0.0879 Validation Accuracy: 0.729800\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss:     0.0383 Validation Accuracy: 0.743800\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:     0.0255 Validation Accuracy: 0.745800\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss:     0.1279 Validation Accuracy: 0.697600\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss:     0.0145 Validation Accuracy: 0.719200\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss:     0.0295 Validation Accuracy: 0.711200\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss:     0.0109 Validation Accuracy: 0.740200\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:     0.0104 Validation Accuracy: 0.748200\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss:     0.0817 Validation Accuracy: 0.696200\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss:     0.0507 Validation Accuracy: 0.737200\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss:     0.0159 Validation Accuracy: 0.728000\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss:     0.0050 Validation Accuracy: 0.741800\n"
     ]
    }
   ],
   "source": [
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "                \n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gZkLP35qjW3T"
   },
   "source": [
    "\n",
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cXVWWfyhjW3U"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "def batch_features_labels(features, labels, batch_size):\n",
    "    \"\"\"\n",
    "    Split features and labels into batches\n",
    "    \"\"\"\n",
    "    for start in range(0, len(features), batch_size):\n",
    "        end = min(start + batch_size, len(features))\n",
    "        yield features[start:end], labels[start:end]\n",
    "\n",
    "def display_image_predictions(features, labels, predictions, top_n_predictions):\n",
    "    n_classes = 10\n",
    "    label_names = load_label_names()\n",
    "    label_binarizer = LabelBinarizer()\n",
    "    label_binarizer.fit(range(n_classes))\n",
    "    label_ids = label_binarizer.inverse_transform(np.array(labels))\n",
    "\n",
    "    fig, axies = plt.subplots(nrows=top_n_predictions, ncols=2, figsize=(20, 10))\n",
    "    fig.tight_layout()\n",
    "    fig.suptitle('Softmax Predictions', fontsize=20, y=1.1)\n",
    "\n",
    "    n_predictions = 3\n",
    "    margin = 0.05\n",
    "    ind = np.arange(n_predictions)\n",
    "    width = (1. - 2. * margin) / n_predictions\n",
    "   \n",
    "    for image_i, (feature, label_id, pred_indicies, pred_values) in enumerate(zip(features, label_ids, predictions.indices, predictions.values)):\n",
    "        if (image_i < top_n_predictions):\n",
    "            pred_names = [label_names[pred_i] for pred_i in pred_indicies]\n",
    "            correct_name = label_names[label_id]\n",
    "            \n",
    "            axies[image_i][0].imshow((feature*255).astype(np.int32, copy=False))\n",
    "            axies[image_i][0].set_title(correct_name)\n",
    "            axies[image_i][0].set_axis_off()\n",
    "\n",
    "            axies[image_i][1].barh(ind + margin, pred_values[:3], width)\n",
    "            axies[image_i][1].set_yticks(ind + margin)\n",
    "            axies[image_i][1].set_yticklabels(pred_names[::-1])\n",
    "            axies[image_i][1].set_xticks([0, 0.5, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "53LthdDNjW3W",
    "outputId": "b5c21493-e5a2-4494-b771-1912c90367e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./image_classification\n",
      "Testing Accuracy: 0.7360668789808917\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "batch_size = 64\n",
    "n_samples = 10\n",
    "top_n_predictions = 5\n",
    "\n",
    "def test_model():\n",
    "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('input_x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('output_y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "    \n",
    "\n",
    "test_model()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CIFAR10_image_classification.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
